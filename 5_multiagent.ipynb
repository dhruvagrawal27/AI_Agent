{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import operator\n",
    "from functools import reduce\n",
    "from typing import Annotated, List, Dict, TypedDict, Literal, Optional, Callable, Set, Tuple, Any, Union, TypeVar\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "from operator import add\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "# Core imports\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, Graph, END, START\n",
    "\n",
    "\n",
    "# Pretty Markdown Output\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich import box\n",
    "from rich.style import Style\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ðŸ”¹ Load API Key from .env\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"NEMOTRON_4_340B_INSTRUCT_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    console.print(\"[bold red]âŒ API key not found! Please set it in .env[/bold red]\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T')\n",
    "\n",
    "def dict_reducer(dict1: Dict[str, Any], dict2: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Merge two dictionaries recursively\n",
    "\n",
    "    Example:\n",
    "    dict1 = {\"a\": {\"x\": 1}, \"b\": 2}\n",
    "    dict2 = {\"a\": {\"y\": 2}, \"c\": 3}\n",
    "    result = {\"a\": {\"x\": 1, \"y\": 2}, \"b\": 2, \"c\": 3}\n",
    "    \"\"\"\n",
    "    merged = dict1.copy()\n",
    "    for key, value in dict2.items():\n",
    "        if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):\n",
    "            merged[key] = dict_reducer(merged[key], value)\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcademicState(TypedDict):\n",
    "    \"\"\"Master state container for the academic assistance system\"\"\"\n",
    "    #  messages: Annotated[List[BaseMessage], add]   # Conversation history\n",
    "    #  profile: dict                                 # Student information\n",
    "    #  calendar: dict                                # Scheduled events\n",
    "    #  tasks: dict                                   # To-do items and assignments\n",
    "    #  results: Dict[str, Any]                       # Operation outputs\n",
    "    messages: Annotated[List[BaseMessage], add]   # Conversation history\n",
    "    profile: Annotated[Dict, dict_reducer]                 # Student information\n",
    "    calendar: Annotated[Dict, dict_reducer]                # Scheduled events\n",
    "    tasks: Annotated[Dict, dict_reducer]                   # To-do items and assignments\n",
    "    results: Annotated[Dict[str, Any], dict_reducer]       # Operation outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Initialization\n",
    "Key Differences:\n",
    "\n",
    "1. Concurrency Model\n",
    "  - AsyncOpenAI: Asynchronous operations using `async/await`\n",
    "  - OpenAI: Synchronous operations that block execution\n",
    "\n",
    "2. Use Cases\n",
    "  - AsyncOpenAI: High throughput, non-blocking operations\n",
    "  - OpenAI: Simple sequential requests, easier debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMConfig:\n",
    "    \"\"\"Settings for NeMo AI model.\"\"\"\n",
    "    base_url: str = \"https://integrate.api.nvidia.com/v1\"\n",
    "    model: str = \"nvidia/nemotron-4-340b-instruct\"\n",
    "    max_tokens: int = 1024\n",
    "    default_temp: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeMoLLaMa:\n",
    "  \"\"\"\n",
    "  A class to interact with NVIDIA's nemotron-4-340b-instruct model through their API\n",
    "  This implementation uses AsyncOpenAI client for asynchronous operations\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, api_key: str):\n",
    "    \"\"\"Initialize NeMoLLaMa with API key.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): NVIDIA API authentication key\n",
    "    \"\"\"\n",
    "    self.config = LLMConfig()\n",
    "    self.client = AsyncOpenAI(\n",
    "        base_url=self.config.base_url,\n",
    "        api_key=api_key\n",
    "    )\n",
    "    self._is_authenticated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_auth(self) -> bool:\n",
    "      \"\"\"Verify API authentication with test request.\n",
    "\n",
    "      Returns:\n",
    "          bool: Authentication status\n",
    "\n",
    "      Example:\n",
    "          >>> is_valid = await llm.check_auth()\n",
    "          >>> print(f\"Authenticated: {is_valid}\")\n",
    "      \"\"\"\n",
    "      test_message = [{\"role\": \"user\", \"content\": \"test\"}]\n",
    "      try:\n",
    "          await self.agenerate(test_message, temperature=0.1)\n",
    "          self._is_authenticated = True\n",
    "          return True\n",
    "      except Exception as e:\n",
    "          print(f\"âŒ Authentication failed: {str(e)}\")\n",
    "          return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agenerate(\n",
    "      self,\n",
    "      messages: List[Dict],\n",
    "      temperature: Optional[float] = None\n",
    "  ) -> str:\n",
    "      \"\"\"Generate text using NeMo LLaMa model.\n",
    "\n",
    "      Args:\n",
    "          messages: List of message dicts with 'role' and 'content'\n",
    "          temperature: Sampling temperature (0.0 to 1.0, default from config)\n",
    "\n",
    "      Returns:\n",
    "          str: Generated text response\n",
    "\n",
    "      Example:\n",
    "          >>> messages = [\n",
    "          ...     {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "          ...     {\"role\": \"user\", \"content\": \"Plan my study schedule\"}\n",
    "          ... ]\n",
    "          >>> response = await llm.agenerate(messages, temperature=0.7)\n",
    "      \"\"\"\n",
    "      completion = await self.client.chat.completions.create(\n",
    "          model=self.config.model,\n",
    "          messages=messages,\n",
    "          temperature=temperature or self.config.default_temp,\n",
    "          max_tokens=self.config.max_tokens,\n",
    "          stream=False\n",
    "      )\n",
    "      return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataManager\n",
    "A centralized data management system for AI agents to handle multiple data sources.\n",
    "\n",
    "This class serves as a unified interface for accessing and managing different types of\n",
    "structured data (profiles, calendars, tasks) that an AI agent might need to process.\n",
    "It handles data loading, parsing, and provides methods for intelligent filtering and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataManager:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize data storage containers.\n",
    "        All data sources start as None until explicitly loaded through load_data().\n",
    "        \"\"\"\n",
    "        self.profile_data = None\n",
    "        self.calendar_data = None\n",
    "        self.task_data = None\n",
    "\n",
    "    def load_data(self, profile_json: str, calendar_json: str, task_json: str):\n",
    "        \"\"\"\n",
    "        Load and parse multiple JSON data sources simultaneously.\n",
    "\n",
    "        Args:\n",
    "            profile_json (str): JSON string containing user profile information\n",
    "            calendar_json (str): JSON string containing calendar events\n",
    "            task_json (str): JSON string containing task/todo items\n",
    "\n",
    "        Note: This method expects valid JSON strings. Any parsing errors will propagate up.\n",
    "        \"\"\"\n",
    "        self.profile_data = json.loads(profile_json)\n",
    "        self.calendar_data = json.loads(calendar_json)\n",
    "        self.task_data = json.loads(task_json)\n",
    "\n",
    "    def get_student_profile(self, student_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Retrieve a specific student's profile using their unique identifier.\n",
    "\n",
    "        Args:\n",
    "            student_id (str): Unique identifier for the student\n",
    "\n",
    "        Returns:\n",
    "            Dict: Student profile data if found, None otherwise\n",
    "\n",
    "        Implementation Note:\n",
    "            Uses generator expression with next() for efficient search through profiles,\n",
    "            avoiding full list iteration when possible.\n",
    "        \"\"\"\n",
    "        if self.profile_data:\n",
    "            return next((p for p in self.profile_data[\"profiles\"]\n",
    "                        if p[\"id\"] == student_id), None)\n",
    "        return None\n",
    "\n",
    "    def parse_datetime(self, dt_str: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Smart datetime parser that handles multiple formats and ensures UTC timezone.\n",
    "\n",
    "        Args:\n",
    "            dt_str (str): DateTime string in ISO format, with or without timezone\n",
    "\n",
    "        Returns:\n",
    "            datetime: Parsed datetime object in UTC timezone\n",
    "\n",
    "        Implementation Note:\n",
    "            Handles both timezone-aware and naive datetime strings by:\n",
    "            1. First attempting to parse with timezone information\n",
    "            2. Falling back to assuming UTC if no timezone is specified\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First attempt: Parse ISO format with timezone\n",
    "            dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))\n",
    "            return dt.astimezone(timezone.utc)\n",
    "        except ValueError:\n",
    "            # Fallback: Assume UTC if no timezone provided\n",
    "            dt = datetime.fromisoformat(dt_str)\n",
    "            return dt.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    def get_upcoming_events(self, days: int = 7) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Intelligently filter and retrieve upcoming calendar events within a specified timeframe.\n",
    "\n",
    "        Args:\n",
    "            days (int): Number of days to look ahead (default: 7)\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of upcoming events, chronologically ordered\n",
    "\n",
    "        Implementation Note:\n",
    "            - Uses UTC timestamps for consistent timezone handling\n",
    "            - Implements error handling for malformed event data\n",
    "            - Only includes events that start in the future up to the specified timeframe\n",
    "        \"\"\"\n",
    "        if not self.calendar_data:\n",
    "            return []\n",
    "\n",
    "        now = datetime.now(timezone.utc)\n",
    "        future = now + timedelta(days=days)\n",
    "\n",
    "        events = []\n",
    "        for event in self.calendar_data.get(\"events\", []):\n",
    "            try:\n",
    "                start_time = self.parse_datetime(event[\"start\"][\"dateTime\"])\n",
    "\n",
    "                if now <= start_time <= future:\n",
    "                    events.append(event)\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(f\"Warning: Could not process event due to {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return events\n",
    "\n",
    "    def get_active_tasks(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve and filter active tasks, enriching them with parsed datetime information.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of active tasks with parsed due dates\n",
    "\n",
    "        Implementation Note:\n",
    "            - Filters for tasks that are:\n",
    "              1. Not completed (\"needsAction\" status)\n",
    "              2. Due in the future\n",
    "            - Enriches task objects with parsed datetime for easier processing\n",
    "            - Implements robust error handling for malformed task data\n",
    "        \"\"\"\n",
    "        if not self.task_data:\n",
    "            return []\n",
    "\n",
    "        now = datetime.now(timezone.utc)\n",
    "        active_tasks = []\n",
    "\n",
    "        for task in self.task_data.get(\"tasks\", []):\n",
    "            try:\n",
    "                due_date = self.parse_datetime(task[\"due\"])\n",
    "                if task[\"status\"] == \"needsAction\" and due_date > now:\n",
    "                    # Enrich task object with parsed datetime\n",
    "                    task[\"due_datetime\"] = due_date\n",
    "                    active_tasks.append(task)\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(f\"Warning: Could not process task due to {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return active_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.NeMoLLaMa object at 0x000001D9FBD9DD60>\n"
     ]
    }
   ],
   "source": [
    "llm = NeMoLLaMa(API_KEY)\n",
    "data_manager = DataManager()\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Executor\n",
    "Orchestrates the concurrent execution of multiple specialized AI agents.\n",
    "\n",
    "This class implements a sophisticated execution pattern that allows multiple AI agents\n",
    "to work together, either sequentially or concurrently, based on a coordination analysis.\n",
    "It handles agent initialization, concurrent execution, error handling, and fallback strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
